---
title: "Pretty big data... now what?"
slug: "formats"
author: "Laura DeCicco"
date: "2018-08-10"
image: "static/formats/visualizeBox-1.png"
keyword1: files
keyword2: io
output: USGSmarkdowntemplates::hugo
author_email: "ldecicco@usgs.gov"
author_github: "ldecicco-usgs"
author_twitter: "DeCiccoDonk"
author_gs: "jXd0feEAAAAJ"
author_staff: "laura-decicco"
description: "Exploring file format options in R."
---

```{r setup, include=FALSE}
library(knitr)

knit_hooks$set(plot=function(x, options) {
  sprintf("<img src='/%s%s-%d.%s'/ title='%s' alt='%s' />",
          options$fig.path, options$label,
          options$fig.cur, options$fig.ext,
          options$fig.cap, options$alt.text)

})


opts_chunk$set(
  echo=TRUE,
  fig.path="static/formats/",
  fig.cap = "TODO",
  alt.text = "TODO",
  fig.width = 7,
  fig.height = 7,
  class = "",
  message = FALSE,
  warnings = FALSE
)
```

In the group that I work with [https://owi.usgs.gov/datascience/](https://owi.usgs.gov/datascience/), the vast majority of the projects use flat files for data storage. Sometimes, the files get a bit large, so we create a set of files...but basically we've been fine without wading into the world of databases. Recently however, the data involved in our projects are creeping up to be bigger and bigger. We're still not anywhere in the "BIG DATA (TM)" realm, but big enough to warrant exploring options. This blog explores the options: csv, RDS, fst, sqlite, and feather. One of the takeaways I've learned was that there is not a single right answer. This post will attempt to lay out the options and summarize the pros and cons. 

In a nearly parallel blog post:  [sqlite-feather-and-fst](https://kbroman.org/blog/2017/04/30/sqlite-feather-and-fst/), Karl Broman discusses his journey from flat files to "big-ish data". I've taken his workflow, added a more robust test for `fst`, and used my own data. 

First question: should we set up a shared database?

# Shared Database

A database is probably many data scientist's go-to tool for data storage and access. There are many database options, and discussing the pros and cons of each could (and does!) fill a semester-long college course. This post will not cover those topics. 

Our initial question was: when should we even *consider* going through the process of setting up a shared database? There's overhead involved, and our group would either need a spend a fair amount of time getting over the initial learning-curve or spend a fair amount of our limited resources on access to skilled data base administrators. None of these hurdles are insurmountable, but we want to make sure our project and data needs are worth those investments. 

If a single file can be easily passed around to coworkers, and loaded entirely in memory directly in R, there doesn't seem to be any reason to consider a shared database. Maybe the data can be logically chunked into several files (or 100's....or 1,000's) that make collaborating on the same data easier. What conditions warrant our "R-flat-file-happy" group to consider a database?

Not being an expert, I asked and got great advice from members of the [rOpenSci](https://ropensci.org/) community. This is what I learned:

* "Identify how much CRUD (create, read, update, delete) you need to do over time and how complicated your conceptual model is. If you need people to be interacting and changing data a shared database can help add change tracking and important constraints to data inputs. If you have multiple things that interact like sites, species, field people, measurement classes, complicated date concepts etc then the db can help." (Steph Locke)

* "One thing to consider is whether the data are updated and how and by single or multiple processes." (Elin Waring)

* "I encourage people towards databases when/if they need to make use of all the validation logic you can put into databases. If they just need to query, a pattern I like is to keep the data in a text-based format like CSV/TSV and load the data into sqlite for querying." (Bryce Mecum)

* "I suppose another criterion is whether multiple people need to access the same data or just a solo user. Concurrency afforded by DBs is nice in that regard." (James Balamuta)

All great points! In the majority of our data science projects, the focus is not on creating and maintaining complex data systems...it's using large amounts of data. Most if not all of that data already come from other databases (usually through web services). So...the big benefits for setting up a shared database for our projects at the moment seems unnecessary. 

# Now what?

OK, so we don't need to buy an Oracle license. We still want to make a smart choice in the way we save and access the data. We usually have 1-to-many file(s) that we share between a few people. So, we'll want to minimize the file size to reduce that transfer time (we have used Google drive and S3 buckets to store files to share historically). We'd also like to minimize the time to read and write the files. Maintaining attributes (like column types) is also ideal.

I will be using a large data frame to test `data.table`, `fst`, `feather`, and `sqlite`. What is in the data frame is not important to this analysis. Keep in mind your own personal "biggish" data frame and your hardware might have different results. 

Let's start by loading the whole file into memory. The columns are a mix of factors, characters, numerics, dates, and logicals.

```{r getData}
biggish <- readRDS("test.rds")

nrow(biggish)
ncol(biggish)

```

Using the "biggish" data frame, I'm going to write and read the files completely in memory to start.

```{r readWrite, warning=FALSE, message=FALSE}
tested <- c("rds","fread","feather","fst","sqlite")
file_size <- setNames(rep(NA,length(tested)), tested)
write_time <- setNames(rep(NA,length(tested)), tested)
read_time <- setNames(rep(NA,length(tested)), tested)

# RDS:
write_time[["rds"]] <- system.time(saveRDS(biggish, "test.rds"))[["elapsed"]]
read_time[["rds"]] <- system.time(rds_df <- readRDS("test.rds"))[["elapsed"]]
file_size[["rds"]] <- file.info("test.rds")[["size"]]

# data.table:
library(data.table)
write_time[["fread"]] <- system.time(fwrite(biggish, "test.csv"))[["elapsed"]]
read_time[["fread"]] <- system.time(fread_df <- fread("test.csv"))[["elapsed"]]
file_size[["fread"]] <- file.info("test.csv")[["size"]]

# feather:
library(feather)
write_time[["feather"]] <- system.time(write_feather(biggish, "test.feather"))[["elapsed"]]
read_time[["feather"]] <- system.time(feather_df <- read_feather("test.feather"))[["elapsed"]]
file_size[["feather"]] <- file.info("test.feather")[["size"]]

# fst:
library(fst)
write_time[["fst"]] <- system.time(write_fst(biggish, "test.fst", compress = 100))[["elapsed"]]
read_time[["fst"]] <- system.time(fst_df <- read.fst("test.fst"))[["elapsed"]]
file_size[["fst"]] <- file.info("test.fst")[["size"]]

# sqlite:
library(RSQLite)
sqlite_file <- "test.sqlite"
sqldb <- dbConnect(SQLite(), dbname=sqlite_file)
write_time[["sqlite"]] <- system.time({
  dbWriteTable(sqldb,name =  "test", biggish, 
               row.names=FALSE, overwrite=TRUE,
               append=FALSE, field.types=NULL)
})[["elapsed"]]
read_time[["sqlite"]] <- system.time(sql_df <- dbGetQuery(sqldb, "select * from test"))[["elapsed"]]
dbDisconnect(sqldb)
file_size[["sqlite"]] <- file.info("test.sqlite")[["size"]]



knitr::kable(data.frame(file_size/10^6,
                        write_time,
                        read_time), 
             digits = c(1,0,1,1),col.names = c("File Size","Write Time", "Read Time"))

```


# Disclaimer

Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government.
