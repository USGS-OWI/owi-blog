---
title: "Pretty big data... now what?"
slug: "formats"
author: "Laura DeCicco"
date: "2018-11-07"
always_allow_html: yes
image: "static/comparison.jpg"
keyword1: files
keyword2: io
output: USGSmarkdowntemplates::hugo
author_email: "ldecicco@usgs.gov"
author_github: "ldecicco-usgs"
author_twitter: "DeCiccoDonk"
author_gs: "jXd0feEAAAAJ"
author_staff: "laura-decicco"
description: "Exploring file format options in R."
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr)

knit_hooks$set(plot=function(x, options) {
  sprintf("<img src='/%s%s-%d.%s'/ title='%s' alt='%s' />",
          options$fig.path, options$label,
          options$fig.cur, options$fig.ext,
          options$fig.cap, options$alt.text)

})


opts_chunk$set(
  echo=TRUE,
  fig.path="static/formats/",
  fig.cap = "TODO",
  alt.text = "TODO",
  fig.width = 7,
  fig.height = 7,
  class = "",
  message = FALSE,
  warnings = FALSE
)
```

In the group that I work with [https://owi.usgs.gov/datascience/](https://owi.usgs.gov/datascience/), the vast majority of the projects use flat files for data storage. Sometimes, the files get a bit large, so we create a set of files...but basically we've been fine without wading into the world of databases. Recently however, the data involved in our projects are creeping up to be bigger and bigger. We're still not anywhere in the "BIG DATA (TM)" realm, but big enough to warrant exploring options. This blog explores the options: csv (both from `readr` and `data.table`), RDS, fst, sqlite, feather, monetDB. One of the takeaways I've learned was that there is not a single right answer. This post will attempt to lay out the options and summarize the pros and cons. 

In a blog post that laid out similar work:  [sqlite-feather-and-fst](https://kbroman.org/blog/2017/04/30/sqlite-feather-and-fst/) and continued [here](https://kbroman.org/blog/2017/05/11/reading/writing-biggish-data-revisited/), Karl Broman discusses his journey from flat files to "big-ish data". I've taken some of his workflow, added more robust for `fst` and `monetDB`, and used my own data. 

First question: should we set up a shared database?

# Shared Database

A database is probably many data scientist's go-to tool for data storage and access. There are many database options, and discussing the pros and cons of each could (and does!) fill a semester-long college course. This post will not cover those topics. 

Our initial question was: when should we even *consider* going through the process of setting up a shared database? There's overhead involved, and our group would either need a spend a fair amount of time getting over the initial learning-curve or spend a fair amount of our limited resources on access to skilled data base administrators. None of these hurdles are insurmountable, but we want to make sure our project and data needs are worth those investments. 

If a single file can be easily passed around to coworkers, and loaded entirely in memory directly in R, there doesn't seem to be any reason to consider a shared database. Maybe the data can be logically chunked into several files (or 100's....or 1,000's) that make collaborating on the same data easier. What conditions warrant our "R-flat-file-happy" group to consider a database?

Not being an expert, I asked and got great advice from members of the [rOpenSci](https://ropensci.org/) community. This is what I learned:

* "Identify how much CRUD (create, read, uprequested_date, delete) you need to do over time and how complicated your conceptual model is. If you need people to be interacting and changing data a shared database can help add change tracking and important constraints to data inputs. If you have multiple things that interact like sites, species, field people, measurement classes, complicated requested_date concepts etc then the db can help." [Steph Locke](https://twitter.com/TheStephLocke)

* "One thing to consider is whether the data are updated and how, and by single or multiple processes." [Elin Waring](https://twitter.com/ElinWaring)

* "I encourage people towards databases when/if they need to make use of all the validation logic you can put into databases. If they just need to query, a pattern I like is to keep the data in a text-based format like CSV/TSV and load the data into sqlite for querying." [Bryce Mecum](https://twitter.com/brycem)

* "I suppose another criterion is whether multiple people need to access the same data or just a solo user. Concurrency afforded by DBs is nice in that regard." [James Balamuta](https://twitter.com/axiomsofxyz)

All great points! In the majority of our data science projects, the focus is not on creating and maintaining complex data systems...it's using large amounts of data. Most if not all of that data already come from other databases (usually through web services). So...the big benefits for setting up a shared database for our projects at the moment seems unnecessary. 

# Now what?

OK, so we don't need to buy an Oracle license. We still want to make a smart choice in the way we save and access the data. We usually have 1-to-many file(s) that we share between a few people. So, we'll want to minimize the file size to reduce that transfer time (we have used Google drive and S3 buckets to store files to share historically). We'd also like to minimize the time to read and write the files. Maintaining attributes (like column types) is also ideal.

I will be using a large data frame to test `data.table`,`readr`, `fst`, `feather`, `sqlite`, and `MonetDBLite`. Late in the game, I tried to incorporate `sparklyr` into this analysis. `sparklyr` looks and sounds like an appealing option especially for "really big data". I was not able to get my standard examples presented here to work however. Also, the dependency on a specific version of Java made me nervous (at least, at the time of writing this blog post). So, while it might be an attractive solution, there was a bit too much of a learning curve for the needs of our group.

What is in the data frame is not important to this analysis. Keep in mind your own personal "biggish" data frame and your hardware might have different results. 

Let's start by loading the whole file into memory. The columns are a mix of factors, characters, numerics, requested_dates, and logicals.

```{r getData, cache=TRUE}
biggish <- readRDS("test.rds")

nrow(biggish)
ncol(biggish)

```

## Read, write, and files size

Using the "biggish" data frame, I'm going to write and read the files completely in memory to start. Because we are often shuffling files around (one person pushes up to an S3 bucket and another pulls them down for example), I also want to compare compressed files vs not compressed when possible.

If you can read in all your data at once, read/write time and file size should be enough to help you choose your file format. There are many instances in our "biggish" data projects that we don't always need nor want ALL the data ALL the time. 

I will also compare how long it takes to pull a subset of the data by pulling out a requested_date column, numeric, and string, and only rows that have specific strings and greater than a threshold. First, I'll show individually how to do each of these operations. The end of this post will contain a table summarizing all the information. It will be generated using the `microbenchmark` package.

## RDS No Compression

We'll start with the basic R binary file, the "RDS" file. `saveRDS` has an argument "compress" that defaults to `TRUE`. Not compressing the files results in a bigger file size, but quicker read and write times.

```{r rds_no_compress, eval=FALSE}
library(dplyr)

file_name <- "test.rds"
# Write:
saveRDS(biggish, file = file_name, compress = FALSE)
# Read:
rds_df <- readRDS(file_name)
```

RDS files must be read entirely in memory so partial read times are not applicable. However, I will use 2 examples throughout to test the timings. The examples are deliberately set up to test many `dplyr` basic verbs and various data types, and other tricky situations like timezones.

```{r ds_no_compress_grouping, eval=FALSE}
min_bytes <- 100000
param_cd <- "00060"
group_col <- "statecd"
service <- "dv"

# Partial Read:
partial_data <- readRDS(file_name) %>%
  filter(bytes > !!min_bytes,
         grepl(!!param_cd, parametercds)) %>%
  select(bytes, requested_date, parametercds)

# Group and summarize:
group_summary <- readRDS(file_name) %>%
  filter(service == !!service, 
         !is.na(!!sym(group_col)),
         requested_date > as.POSIXct("2016-10-02 00:00:00", 
                                     tz = "America/New_York")) %>%
  mutate(requested_date = as.Date(requested_date)) %>%
  group_by(.dots = c(group_col, "requested_date")) %>%
  summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6)
```

Timing in seconds:

```{r rds_no_timing, echo=FALSE, cache=TRUE}
library(microbenchmark)
library(dplyr)

times <- 10
min_bytes <- 100000
param_cd <- "00060"
group_col <- "statecd"
service <- "dv"
file_name <- "test.rds"

timing_rds <- microbenchmark(
  write = saveRDS(biggish, file = file_name, compress = FALSE),
  read = readRDS(file_name),
  partial = {
    partial_data <- readRDS(file_name) %>%
      filter(bytes > !!min_bytes,
             grepl(!!param_cd, parametercds)) %>%
      select(bytes, requested_date, parametercds)
  },
  grouping = {
    group_summary <- readRDS(file_name) %>%
      filter(service == !!service, 
             !is.na(!!sym(group_col)),
             requested_date > as.POSIXct("2016-10-02 00:00:00", 
                                         tz = "America/New_York")) %>%
      mutate(requested_date = as.Date(requested_date)) %>%
      group_by(.dots = c(group_col, "requested_date")) %>%
      summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6)
  },
  times = times
)

get_seconds <- function(timing, format){
  
  if(as.character(attr(summary(timing), "unit")) != "seconds"){
    unit <- as.character(attr(summary(timing), "unit"))
  
    if(unit == "milliseconds"){
      read_write <- data.frame(
        format = format,
        read = summary(timing)$mean[2]/1000,
        write = summary(timing)$mean[1]/1000,
        partial = summary(timing)$mean[3]/1000,
        grouping = summary(timing)$mean[4]/1000,
        stringsAsFactors = FALSE
      )    
    }
  } else {
    read_write <- data.frame(
      format = format,
      read = summary(timing)$mean[2],
      write = summary(timing)$mean[1],
      partial = summary(timing)$mean[3],
      grouping = summary(timing)$mean[4],
      stringsAsFactors = FALSE
    )  
  }
  
  return(read_write)
}

read_write <- get_seconds(timing_rds,"rds")

knitr::kable(read_write, digits = 1)
```

## RDS Compression

```{r rds_compress, eval=FALSE}
file_name <- "test_compressed.rds"
# Write:
saveRDS(biggish, file = file_name, compress = TRUE)
# Read:
rds_compressed_df <- readRDS(file_name)

```

The partial data files will be the same process as "RDS No Compression". Timing in seconds:

```{r rds_compress_output, echo=FALSE, cache=TRUE}
file_name <- "test_compressed.rds"

timing <- microbenchmark(
  write = saveRDS(biggish, file = file_name, compress = TRUE),
  read = readRDS(file_name),
  partial = {
    partial_data <- readRDS(file_name) %>%
      filter(bytes > !!min_bytes,
             grepl(!!param_cd, parametercds)) %>%
      select(bytes, requested_date, parametercds)
  },
  grouping = {
    group_summary <- readRDS(file_name) %>%
      filter(service == !!service, 
             !is.na(!!sym(group_col)),
             requested_date > as.POSIXct("2016-10-02 00:00:00", 
                                         tz = "America/New_York")) %>%
      mutate(requested_date = as.Date(requested_date)) %>%
      group_by(.dots = c(group_col, "requested_date")) %>%
      summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6)
  },
  times = times
)

read_write_rds_comp <- get_seconds(timing,"rds compression")

read_write <- bind_rows(read_write, read_write_rds_comp)

knitr::kable(read_write_rds_comp, digits = 1)
```

## readr No Compression

```{r readr, eval=FALSE}
library(readr)
file_name <- "test.csv"
# Write:
write_csv(biggish, path = file_name)
# Read:
readr_df <- read_csv(file_name)
attr(readr_df$requested_date, "tzone") <- "America/New_York"
```

`readr` also must be read entirely in memory so partial read times are not applicable. If there's a known, continuous set of rows, you can use the arguments "skip" and "n_max" to pull just what you need. However, that is not flexible enough for most of our needs, so I am not including that in this evaluation.

```{r readr_partial, eval=FALSE}
min_bytes <- 100000
param_cd <- "00060"
group_col <- "statecd"
service <- "dv"

# Partial Read:
partial_data_rdr <- read_csv(file_name) %>%
  filter(bytes > !!min_bytes,
         grepl(!!param_cd, parametercds)) %>%
  select(bytes, requested_date, parametercds)

attr(partial_data_rdr$requested_date, "tzone") <- "America/New_York"

# Group and summarize:
group_summary_rdr <- read_csv(file_name) %>%
  filter(service == !!service, 
         !is.na(!!sym(group_col)),
         requested_date > as.POSIXct("2016-10-02 00:00:00")) %>%
  mutate(requested_date = as.Date(requested_date)) %>%
  group_by(.dots = c(group_col, "requested_date")) %>%
  summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6)

```

Timing in seconds:

```{r readr_no_output, echo=FALSE, cache=TRUE}
library(readr)
file_name <- "test.csv"

timing <- microbenchmark(
  write = write_csv(biggish, path = file_name),
  read = {
    readr_df <- read_csv(file_name)
    attr(readr_df$requested_date, "tzone") <- "America/New_York"
  },
  partial = {
    partial_data_rdr <- read_csv(file_name) %>%
      filter(bytes > !!min_bytes,
             grepl(!!param_cd, parametercds)) %>%
      select(bytes, requested_date, parametercds)
    
    attr(partial_data_rdr$requested_date, "tzone") <- "America/New_York"
  },
  grouping = {
    group_summary_rdr <- read_csv(file_name) %>%
      filter(service == !!service, 
             !is.na(!!sym(group_col)),
             requested_date > as.POSIXct("2016-10-02 00:00:00")) %>%
      mutate(requested_date = as.Date(requested_date)) %>%
      group_by(.dots = c(group_col, "requested_date")) %>%
      summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6)
  },
  times = times
)

read_write_readr <- get_seconds(timing,"readr")

read_write <- bind_rows(read_write, read_write_readr)

knitr::kable(read_write_readr, digits = 1)

```

## readr Compression

```{r readr_compressed, eval=FALSE}
library(readr)
file_name <- "test_readr.csv.gz"
# Write:
write_csv(biggish,  path = file_name)
# Read:
readr_compressed_df <- read_csv(file_name)
```

The partial data files will be the same process as "readr No Compression".

```{r readr_compressed_output, echo=FALSE, cache=TRUE}
file_name <- "test_readr.csv.gz"

timing <- microbenchmark(
  write = write_csv(biggish, path = file_name),
  read = {
    readr_df <- read_csv(file_name)
    attr(readr_df$requested_date, "tzone") <- "America/New_York"
  },
  partial = {
    partial_data_rdr <- read_csv(file_name) %>%
      filter(bytes > !!min_bytes,
             grepl(!!param_cd, parametercds)) %>%
      select(bytes, requested_date, parametercds)
    
    attr(partial_data_rdr$requested_date, "tzone") <- "America/New_York"
  },
  grouping = {
    group_summary_rdr <- read_csv(file_name) %>%
      filter(service == !!service, 
             !is.na(!!sym(group_col)),
             requested_date > as.POSIXct("2016-10-02 00:00:00")) %>%
      mutate(requested_date = as.Date(requested_date)) %>%
      group_by(.dots = c(group_col, "requested_date")) %>%
      summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6)
  },
  times = times
)

read_write_readr_comp <- get_seconds(timing,"readr compression")

read_write <- bind_rows(read_write, read_write_readr_comp)

knitr::kable(read_write_readr_comp, digits = 1)

```

## fread No Compression

```{r fread, eval=FALSE}
library(data.table)
library(fasttime)
file_name <- "test.csv"
# Write:
fwrite(biggish, file = file_name)
# Read:
fread_df <- fread(file_name, 
                  data.table = FALSE, 
                  na.strings = "",) %>%
  mutate(requested_date = fastPOSIXct(requested_date, tz = "America/New_York"))
```

`fread` includes arguments "select"/"drop" to only load specific columns into memory. This improves the load time if there are many columns that aren't needed. If there's a known, continuous set of rows, you can use the arguments "skip" and "nrows" to pull just what you need. However, that is not flexible enough for most of our needs, so I am not including that in this evaluation.

Also, I am keeping this analysis as a "data.frame" (rather than "data.table") because it is the system our group has decided to stick with. 

```{r fread_partial, eval=FALSE}
min_bytes <- 100000
param_cd <- "00060"
group_col <- "statecd"
service <- "dv"

# Partial Read:
partial_data_fread <- fread(file_name, na.strings = "",
                            data.table = FALSE,
                            select = c("bytes","requested_date","parametercds")) %>%
  filter(bytes > !!min_bytes,
         grepl(!!param_cd, parametercds)) %>%
  mutate(requested_date = fastPOSIXct(requested_date, tz = "America/New_York"))

# Group and summarize:
group_summary_fread <- fread(file_name,na.strings = "",
                             data.table = FALSE,
                             select = c("bytes","requested_date","service",group_col)) %>%
  mutate(requested_date = fastPOSIXct(requested_date, 
                                      tz = "America/New_York")) %>%
  filter(service == !!service, 
         !is.na(!!sym(group_col)),
         requested_date > as.POSIXct("2016-10-02 00:00:00")) %>%
  mutate(requested_date = as.Date(requested_date)) %>%
  group_by(.dots = c(group_col, "requested_date")) %>%
  summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6)

```

```{r fread_output, echo=FALSE, cache=TRUE}
library(data.table)
library(fasttime)
file_name <- "test.csv"

timing <- microbenchmark(
  write = fwrite(biggish, file = file_name),
  read = {
    fread_df <- fread(file_name, 
                      data.table = FALSE, 
                      na.strings = "",) %>%
      mutate(requested_date = fastPOSIXct(requested_date, 
                                          tz = "America/New_York"))
  },
  partial = {
    partial_data_fread <- fread(file_name, na.strings = "",
                                data.table = FALSE,
                                select = c("bytes","requested_date","parametercds")) %>%
      filter(bytes > !!min_bytes,
             grepl(!!param_cd, parametercds)) %>%
      mutate(requested_date = fastPOSIXct(requested_date, tz = "America/New_York"))
  },
  grouping = {
    group_summary_fread <- fread(file_name,na.strings = "",
                                 data.table = FALSE,
                                 select = c("bytes","requested_date","service",group_col)) %>%
      mutate(requested_date = fastPOSIXct(requested_date, 
                                          tz = "America/New_York")) %>%
      filter(service == !!service, 
             !is.na(!!sym(group_col)),
             requested_date > as.POSIXct("2016-10-02 00:00:00")) %>%
      mutate(requested_date = as.Date(requested_date)) %>%
      group_by(.dots = c(group_col, "requested_date")) %>%
      summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6)
  },
  times = times
)

read_write_fread <- get_seconds(timing,"fread")

read_write <- bind_rows(read_write, read_write_fread)

knitr::kable(read_write_fread, digits = 1)

```

Note! I didn't explore adjusting the `nThread` argument in `fread`/`fwrite`. I also didn't include a compressed version of `fread`/`fwrite`. Our crew is a hog-pog of Windows, Mac, and Linux, and we try to make our code work on any OS. Many of the solutions for combining compression with `data.table` functions looked fragile on the different OS. 

The `data.table` package has an open GitHub issues to support compression in the future. It may be worth updating this script once that is added.

## feather Compression

```{r feather, eval=FALSE}
library(feather)
file_name <- "test.feather"
# Write:
write_feather(biggish, path = file_name)
# Read:
feather_df <- read_feather(file_name)
```

`read_feather` includes an argument "columns" to only load specific columns into memory. This improves the load time if there are many columns that aren't needed. 

```{r feather_partial, eval=FALSE}
min_bytes <- 100000
param_cd <- "00060"
group_col <- "statecd"
service <- "dv"

# Partial Read:
partial_data_feather <- read_feather(file_name, 
        columns = c("bytes","requested_date","parametercds")) %>%
  filter(bytes > !!min_bytes,
         grepl(!!param_cd, parametercds))

# Group and summarize:
group_summary_feather <- read_feather(file_name, 
                      columns = c("bytes","requested_date","service",group_col)) %>%
  filter(service == !!service, 
         !is.na(!!sym(group_col)),
         requested_date > as.POSIXct("2016-10-02 00:00:00", 
       tz = "America/New_York")) %>%
  mutate(requested_date = as.Date(requested_date)) %>%
  group_by(.dots = c(group_col, "requested_date")) %>%
  summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6)

```

Timing in seconds:

```{r feather_output, echo=FALSE, cache=TRUE}
library(feather)
file_name <- "test.feather"

timing <- microbenchmark(
  write = write_feather(biggish, path = file_name),
  read = {
    feather_df <- read_feather(file_name)
  },
  partial = {
    partial_data_feather <- read_feather(file_name, 
            columns = c("bytes","requested_date","parametercds")) %>%
      filter(bytes > !!min_bytes,
             grepl(!!param_cd, parametercds))
  },
  grouping = {
    group_summary_feather <- read_feather(file_name, 
                          columns = c("bytes","requested_date","service",group_col)) %>%
      filter(service == !!service, 
             !is.na(!!sym(group_col)),
             requested_date > as.POSIXct("2016-10-02 00:00:00", 
           tz = "America/New_York")) %>%
      mutate(requested_date = as.Date(requested_date)) %>%
      group_by(.dots = c(group_col, "requested_date")) %>%
      summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6)
  },
  times = times
)

read_write_feather <- get_seconds(timing,"feather")

read_write <- bind_rows(read_write, read_write_feather)

knitr::kable(read_write_feather, digits = 1)

```

For the same reason as `fread`, I didn't try compressing the `feather` format. Both `data.table` and `feather` have open GitHub issues to support compression in the future. It may be worth updating this script once those features are added. 

## fst No Compression

```{r fst, eval=FALSE}
library(fst)
file_name <- "test.fst"

# Write:
write_fst(biggish, path = file_name, compress = 0)
# Read:
fst_df <- read_fst(file_name)
```

```{r fst_partial, eval=FALSE}
min_bytes <- 100000
param_cd <- "00060"
group_col <- "statecd"
service <- "dv"

# Partial Read:
partial_data_fst <- read_fst(file_name, 
        columns = c("bytes","requested_date","parametercds")) %>%
  filter(bytes > !!min_bytes,
         grepl(!!param_cd, parametercds)) 

# Group and summarize:
group_summary_fst <- read_fst(file_name, 
                      columns = c("bytes","requested_date","service",group_col)) %>%
  filter(service == !!service, 
         !is.na(!!sym(group_col)),
         requested_date > as.POSIXct("2016-10-02 00:00:00", 
       tz = "America/New_York")) %>%
  mutate(requested_date = as.Date(requested_date)) %>%
  group_by(.dots = c(group_col, "requested_date")) %>%
  summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6)

```

Timing in seconds:

```{r fst_output, echo=FALSE, cache=TRUE}
library(fst)
file_name <- "test.fst"

timing <- microbenchmark(
  write = write_fst(biggish, path = file_name, compress = 0),
  read = {
    fst_df <- read_fst(file_name)
  },
  partial = {
    partial_data_fst <- read_fst(file_name, 
            columns = c("bytes","requested_date","parametercds")) %>%
      filter(bytes > !!min_bytes,
             grepl(!!param_cd, parametercds)) 
  },
  grouping = {
    group_summary_fst <- read_fst(file_name, 
                          columns = c("bytes","requested_date","service",group_col)) %>%
      filter(service == !!service, 
             !is.na(!!sym(group_col)),
             requested_date > as.POSIXct("2016-10-02 00:00:00", 
           tz = "America/New_York")) %>%
      mutate(requested_date = as.Date(requested_date)) %>%
      group_by(.dots = c(group_col, "requested_date")) %>%
      summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6) 
  },
  times = times
)

read_write_fst <- get_seconds(timing,"fst")

read_write <- bind_rows(read_write, read_write_fst)

knitr::kable(read_write_fst, digits = 1)

```


## fst Compression

```{r fst_comp, eval=FALSE}
library(fst)
file_name <- "test_compressed.fst"

# Write:
write_fst(biggish, path = file_name, compress = 100)
# Read:
fst_df <- read_fst(file_name)
```

The partial data files will be the same process as "fst No Compression". Timing in seconds:

```{r fst_comp_output, echo=FALSE, cache=TRUE}
file_name <- "test_compressed.fst"

timing <- microbenchmark(
  write = write_fst(biggish, path = file_name, compress = 100),
  read = {
    fst_df <- read_fst(file_name)
  },
  partial = {
    partial_data_fst <- read_fst(file_name, 
            columns = c("bytes","requested_date","parametercds")) %>%
      filter(bytes > !!min_bytes,
             grepl(!!param_cd, parametercds)) 
  },
  grouping = {
    group_summary_fst <- read_fst(file_name, 
                          columns = c("bytes","requested_date","service",group_col)) %>%
      filter(service == !!service, 
             !is.na(!!sym(group_col)),
             requested_date > as.POSIXct("2016-10-02 00:00:00", 
           tz = "America/New_York")) %>%
      mutate(requested_date = as.Date(requested_date)) %>%
      group_by(.dots = c(group_col, "requested_date")) %>%
      summarize(MB = sum(as.numeric(bytes), na.rm = TRUE)/10^6) 
  },
  times = times
)

read_write_fst_comp <- get_seconds(timing,"fst compression")

read_write <- bind_rows(read_write, read_write_fst_comp)

knitr::kable(read_write_fst_comp, digits = 1)

```

## SQLite

SQLite does not have a storage class set aside for storing dates and/or times.  

```{r sqlite, eval=FALSE}
library(RSQLite)

file_name <- "test.sqlite"

sqldb <- dbConnect(SQLite(), dbname=file_name)

# Write:
dbWriteTable(sqldb,name =  "test", biggish,
             row.names=FALSE, overwrite=TRUE,
             append=FALSE, field.types=NULL)
# Read:
sqlite_df <- tbl(sqldb,"test") %>% 
  collect() %>%
  mutate(requested_date = as.POSIXct(requested_date, 
                                     tz = "America/New_York",
                                     origin = "1970-01-01"))
 
```

Things to notice here, you can't just use `grep`.

```{r sqlite_partial, eval=FALSE}
min_bytes <- 100000
param_cd <- "00060"
group_col <- "statecd"
service <- "dv"

# Partial Read:
partial_data_sqlite <- tbl(sqldb,"test") %>% 
  select(bytes, requested_date , parametercds) %>%
  filter(bytes > !!min_bytes,
         parametercds %like% '%00060%') %>%
  collect() %>%
  mutate(requested_date = as.POSIXct(requested_date, 
                                     tz = "America/New_York",
                                     origin = "1970-01-01"))

# Group and summarize:
filter_time <- as.numeric(as.POSIXct("2016-10-02 00:00:00", tz = "America/New_York"))

group_summary_sqlite <- tbl(sqldb,"test") %>%
  select(bytes, requested_date, service, !!group_col) %>%
  filter(service == !!service, 
         !is.na(!!sym(group_col)),
         requested_date > !! filter_time) %>%
  mutate(requested_date = strftime('%Y-%m-%d', datetime(requested_date, 'unixepoch'))) %>%
  group_by(!!sym(group_col), requested_date) %>%
  summarize(MB = sum(bytes, na.rm = TRUE)/10^6) %>%
  collect()

dbDisconnect(sqldb)
```

Timing in seconds:

```{r sqlite_output, echo=FALSE, cache=TRUE}
library(RSQLite)

file_name <- "test.sqlite"
sqldb <- dbConnect(SQLite(), dbname=file_name)

timing <- microbenchmark(
  write = {
    dbWriteTable(sqldb,name =  "test", biggish,
             row.names=FALSE, overwrite=TRUE,
             append=FALSE, field.types=NULL)
  },
  read = {
    sqlite_df <- tbl(sqldb,"test") %>% 
      collect() %>%
      mutate(requested_date = as.POSIXct(requested_date, 
                                         tz = "America/New_York",
                                         origin = "1970-01-01"))
  },
  partial = {
    partial_data_sqlite <- tbl(sqldb,"test") %>% 
      select(bytes, requested_date , parametercds) %>%
      filter(bytes > !!min_bytes,
             parametercds %like% '%00060%') %>%
      collect() %>%
      mutate(requested_date = as.POSIXct(requested_date, 
                                         tz = "America/New_York",
                                         origin = "1970-01-01"))
    
  },
  grouping = {
        filter_time <- as.numeric(as.POSIXct("2016-10-02 00:00:00",
                                             tz = "America/New_York")) 
    
    group_summary_sqlite <- tbl(sqldb,"test") %>%
      select(bytes, requested_date, service, !!group_col) %>%
      filter(service == !!service, 
             !is.na(!!sym(group_col)),
             requested_date > !! filter_time) %>%
      mutate(requested_date = strftime('%Y-%m-%d', datetime(requested_date, 'unixepoch'))) %>%
      group_by(!!sym(group_col), requested_date) %>%
      summarize(MB = sum(bytes, na.rm = TRUE)/10^6) %>%
      collect() 
  },
  times = times
)

dbDisconnect(sqldb)

read_write_sqlite <- get_seconds(timing,"sqlite")

read_write <- bind_rows(read_write, read_write_sqlite)

knitr::kable(read_write_sqlite, digits = 1)
```

It is important to note that this is the first "partial" and "grouping" solution that is completely done outside of R. So when you are getting data that pushes the limits (or passes the limits) of what you can load directly into R, this is the first basic solution.

## MonetDB

```{r monet, eval=FALSE}
library(MonetDBLite)
library(DBI)

file_name <- "test.monet"

con <- dbConnect(MonetDBLite(), dbname = file_name)

# Write:
dbWriteTable(con, name =  "test", biggish,
             row.names=FALSE, overwrite=TRUE,
             append=FALSE, field.types=NULL)
# Read:
monet_df <- dbReadTable(con, "test")
attr(monet_df$requested_date, "tzone") <- "America/New_York"

```

```{r monet_partial, eval=FALSE}
min_bytes <- 100000
param_cd <- "00060"
group_col <- "statecd"
service <- "dv"

# Partial Read:
partial_data_monet <- tbl(con,"test") %>% 
  select(bytes, requested_date , parametercds) %>%
  filter(bytes > !!min_bytes,
         parametercds %like% '%00060%') %>%
  collect() 

attr(partial_data_monet$requested_date, "tzone") <- "America/New_York"

# Group and summarize:
# MonetDB needs the time in UTC, formatted exactly as:
# 'YYYY-mm-dd HH:MM:SS', hence the last "format" commend:
filter_time <- as.POSIXct("2016-10-02 00:00:00", tz = "America/New_York")
attr(filter_time, "tzone") <- "UTC"
filter_time <- format(filter_time)

group_summary_monet <- tbl(con,"test") %>%
  select(bytes, requested_date, service, !!group_col) %>%
  filter(service == !!service, 
         !is.na(!!sym(group_col)), 
         requested_date > !! filter_time) %>% 
  mutate(requested_date = str_to_date(timestamp_to_str(requested_date, '%Y-%m-%d'),'%Y-%m-%d')) %>%
  group_by(!!sym(group_col), requested_date) %>%
  summarize(MB = sum(bytes, na.rm = TRUE)/10^6) %>%
  collect()

dbDisconnect(con, shutdown=TRUE)
```

Timing in seconds:

```{r monet_output, echo=FALSE, cache=TRUE}
library(MonetDBLite)
library(DBI)

file_name <- "test.monet"

con <- dbConnect(MonetDBLite(), dbname = file_name)

timing <- microbenchmark(
  write = {
    dbWriteTable(con, name =  "test", biggish,
             row.names=FALSE, overwrite=TRUE,
             append=FALSE, field.types=NULL)
  },
  read = {
    monet_df <- dbReadTable(con, "test")
    attr(monet_df$requested_date, "tzone") <- "America/New_York"
  },
  partial = {
    partial_data_monet <- tbl(con,"test") %>% 
      select(bytes, requested_date , parametercds) %>%
      filter(bytes > !!min_bytes,
             parametercds %like% '%00060%') %>%
      collect() 
    
    attr(partial_data_monet$requested_date, "tzone") <- "America/New_York"
  },
  grouping = {
    
    filter_time <- as.POSIXct("2016-10-02 00:00:00", tz = "America/New_York")
    attr(filter_time, "tzone") <- "UTC"
    filter_time <- format(filter_time)
    
    group_summary_monet <- tbl(con,"test") %>%
        select(bytes, requested_date, service, !!group_col) %>%
        filter(service == !!service, 
               !is.na(!!sym(group_col)), 
               requested_date > !! filter_time) %>% 
        mutate(requested_date = str_to_date(timestamp_to_str(requested_date, '%Y-%m-%d'),'%Y-%m-%d')) %>%
        group_by(!!sym(group_col), requested_date) %>%
        summarize(MB = sum(bytes, na.rm = TRUE)/10^6) %>%
        collect()
  },
  times = times
)

dbDisconnect(con, shutdown=TRUE)

read_write_monet <- get_seconds(timing,"MonetDB")

read_write <- bind_rows(read_write, read_write_monet)

knitr::kable(read_write_monet, digits = 1)

```

Again, it is important to note that the "partial" and "grouping" solutions are completely done outside of R. So when you are getting data that pushes the limits (or passes the limits) of what you can load directly into R, this is another good solution. There also appears to be a lot more flexibility in using date/times directly in MonetDB compared to SQLite.


# Comparison

```{r showIt, echo=FALSE}
library(kableExtra)
library(formattable)
library(dplyr)

tested <- c("rds","rds_compressed",
            "readr","readr_compressed",
            "fread","feather",
            "fst","fst_compressed",
            "sqlite","monetDB"
            )
file_name <- setNames(c("test.rds","test_compressed.rds",
                        "test_readr.csv","test_readr.csv.gz",
                        "test.csv","test.feather",
                        "test.fst", "test_compressed.fst",
                        "test.sqlite","test.montdb"
                        ),tested)
file_size <- c()

for(file_to_measure in names(file_name)){
  file_size[[file_to_measure]] <- file.info(file_name[[file_to_measure]])[["size"]]
}

# MonetDB isn't really 0...it's a folder:
file_size[["monetDB"]] <- sum(file.info(list.files(path = file_name[["monetDB"]],full.names = TRUE, all.files = TRUE, recursive = TRUE, include.dirs = TRUE))$size, na.rm = TRUE)

read_write$file_size <- round(as.numeric(file_size/10^6),digits = 1)

read_write <- read_write %>%
  mutate(partial = round(partial, digits = 1),
         grouping = round(grouping, digits = 1),
         read = round(read, digits = 1),
         write = round(write, digits = 1)) %>%
  mutate(partial = ifelse(format %in% c("MonetDB","sqlite"),
                      cell_spec(partial, "html", color = "green", bold = TRUE),
                      ifelse(format %in% c("fread","feather","fst","fst compression"), 
                          cell_spec(partial, "html", color = "blue", bold = TRUE),
                          cell_spec(partial, "html", color = "red"))),
        grouping = ifelse(format %in% c("MonetDB","sqlite"),
                      cell_spec(grouping, "html", color = "green", bold = TRUE),
                      ifelse(format %in% c("fread","feather","fst","fst compression"), 
                          cell_spec(grouping, "html", color = "blue", bold = TRUE),
                          cell_spec(grouping, "html", color = "red"))))

read_write_table <- read_write %>%
  kable("html", escape = FALSE,align = "c",
        col.names = c("File Format",
                     "Read",
                     "Write",
                     "Partial",
                     "Grouping",
                     "File Size (MB)")) %>%
  kable_styling("hover", full_width = TRUE)

read_write_table
```

Not that `sqlite` and `MonetDB` are the only formats here that allow careful filtering and calculate summaries without loading the whole data set. So if our "pretty big data" gets "really big", those will formats will rise to the top.


If you can read in all the rows without crashing R, `fread`, `feather`, and `fst` are fast!


Another consideration, who are your collaborators? If everyone's using R exclusively, this table on it's own is a fine way to judge what format to pick. If your collaborators are half R, half Python...you might favor `feather` since that format works well in both systems. 

Collecting this information has been a very useful activity for helping me understand the various options for saving and reading data in R. I tried to pick somewhat complex queries to test out the capabilities, but I acknowledge I'm not an expert on especially the database formats. I would be very happy to hear more efficient ways to perform these analyses. 


# Disclaimer

Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government.
